Android视频开发基础(一）
原创 2017年03月16日 00:04:44 标签：视频编辑 /视频处理 /编码格式 /H264 /码率 2278
﻿       最近项目中实现了短视频的功能，主要包括视频的编辑处理、上传和播放。这是第一次真正略深入的接触视频开发，项目中接触到许多新的概念，也踩了很多坑，整体感觉视频处理还是十分复杂的知识体系，需要花大量的精力和时间才能很好的掌握这项技术。
       下面就谈一谈其中涉及到的一些基础内容，也作为对之前工作的回顾。

基本概念
码率
       码率就是数据传输时单位时间传送的数据位数，单位一般是kbps即千位每秒。码率影响体积，与体积成正比：码率越大，体积越大；码率越小，体积越小。码率还影响清晰度，码率越高清晰度也就越高。需要注意的是码率超过一定数值，对图像的质量没有多大影响。通常来讲，体积=码率×时间，由于有其他因素影响，这个值会稍微有一些误差。码率也就是采样率（并不等同于采样率，采样率的单位是Hz，表示每秒采样的次数），单位时间内采样率越大，精度就越高，处理出来的文件就越接近原始文件。但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率）。
       通常来说，一个视频文件包括画面和声音，音频及视频都有各自不同的码率，也就是说，同一个视频文件音频和视频的码率并不相同。而通常所说的视频文件码率大小，一般是指视频文件中音频和视频码率的总和。

       CBR 编码 （Constant Bitrate）
       CBR 编码是一种静态码率，也就是视频文件从头到尾都是恒定码率。
       它的优点是运算量小，压缩快，能被大多数软件和设备支持，缺点是占用空间相对大，压缩出来的视频画质较差，尤其是在在画面剧烈运动的时候会由于码率不够而丢失部分画面信息，从视觉上来看就是画面波纹严重，图像不清晰。一般情况下，合成相同质量的节目时，采用CBR编码方式时节目合成时间会短一些，但文件的长度会大一些。

       VBR 编码 （Variable Bitrate）
       它是一种动态码率，也就是没有固定的比特率，编码算法根据视频内容的变化来选择不同的实时码率对视频进行编码，把较高的码率用于复杂的动态画面，而把较低的码率用于静态画面，合理利用资源，达到画质较高的情况下，体积也较小。当视频内容的画面变化忽大忽小时，VBR 编码是很有优势的。使用 VBR 编码时，系统将自动为内容的简单部分分配较少的比特，从而留出足量的比特用于生成高质量的复杂部分，这意味着复杂性恒定的内容不会受益于 VBR 编码。对混合内容使用 VBR 编码时，在文件大小相同的条件下，VBR 编码的输出结果要比 CBR 编码的输出结果质量好得多。在某些情况下，与 CBR 编码文件质量相同的 VBR 编码文件，其体积大小可能只有前者的一半。
       VBR的优点就体现出来了，画面质量相对高，文件体积相对较小，但是缺点也很明显，编码的算法要复杂很多，运算量大，压缩时间长。

帧率
        帧率或者称FPS（Frames Per Second，帧/秒），是指每秒显示的图片数，或者GPU处理时每秒能够更新的次数。越高的帧速率可以得到更流畅逼真的画面。每秒钟帧数越多，所显示的动作就会越流畅。帧率也会影响体积，帧率越高，每秒钟显示的画面越多，体积就越大。
       由于人类眼睛的特殊生理结构，如果画面帧率高于16，就会认为是连贯的，此现象称之为视觉暂留。并且当帧速达到一定数值后，再增长的话，人眼能感知的流畅度的提升就比较有限了。一般来说30fps是可以接受的，将性能提升至60fps则可以明显提升交互感和逼真感，如果超过75fps一般就不容易察觉到有明显的流畅度提升了。如果帧率超过屏幕刷新率只会浪费图形处理的能力，因为监视器不能以这么快的速度更新。

分辨率
       通常所说的1280x720（720p）或者1920x1080（1080p），就是视频的分辨率。分辨率影响图像大小，与图像大小成正比：分辨率越高，图像越大；分辨率越低，图像越小。

清晰度
       在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰。
       在分辨率一定的情况下，码率与清晰度成正比关系，码率越高，图像越清晰；码率越低，图像越不清晰。

声道
       指声音在录制或播放时，在不同空间位置采集或回放的相互独立的音频信号，所以声道数也就是声音录制时的音源数量或回放时相应的扬声器数量。 

位深度
       每一个像素用多少位表示，这个位数就叫“位深度”。位深度越大，可用的颜色越多，颜色表现就越逼真。位深度为8的图像可用颜色为256种。

       每个视频都包含这些基本的参数，我们可以通过一个叫做MediaInfo的工具在电脑上来查看视频文件的详细参数。下面的两张图展示了一个视频的参数信息。

      
       从上面的图中，可以看到视频的帧率为20fps，视频平均码率为195kbps，而且是采用的VBR编码，而音频和视频分别的码率为64kbps和128kbps，视频轨和音频轨的编码方式分别为H.264 AVC和AAC。

视频的格式
       说到视频的格式，首先想到的可能会是mp4、3gp、avi、flv、rmvb、mkv等日常生活中常见视频文件格式。这些格式其实只是视频的封装格式。事实上视频的格式分为两种：封装格式和编码格式。
       一张图片可以用很多种算法来显示在屏幕中，所以图片的编码方式就有了JPEG，BMP，PNG等。。。 视频是动态图像，所以在这里就又多出了一个概念，用什么样的方式表达1s内的若干张图片，这就是简单意义上的视频编码，继而就诞生了MPEG，H.264等多种编码格式。后来有了音频、字幕、配音，就需要一个容器来把他们全部放进去保存，所以就诞生了封装格式。这段话就通俗的解释了编码格式和封装格式的作用和区别。

封装格式
       所谓封装格式，就是按照一定的规则来将视频轨道、可能多个的音频轨道、字幕等视频文件所需要的零散信息组装在一起，播放视频文件的时候一起同步进行播放，并且提供了视频索引，可以让你在播放视频时拖动进度条。也就是说它仅仅是一个外壳，或者把它当成一个放视频轨和音频轨的文件夹也可以。同时里面都会有一些信息，比如当前流中包含哪些编码类型、时间戳等，播放器可以按照这些信息来匹配解码器、同步音视频。但是视频文件的封装格式并不影响视频的画质，影响视频画质的是视频的编码格式。
       视频封装格式其实和里面的视频编码和音频编码关系不大。之所以有这么多不同的封装格式是为了适应不同的播放场景。典型的三种视频封装：ts、mp4和mkv，ts适合网络流媒体播放，体积较大，一般用于电视直播或网络直播中；mp4一般只包括一条视频轨和一条音频轨，适合大多数设备，兼容性最好；mkv可以封装多个音频轨、字幕轨，适合网络传播分享。
       大多数情况下，视频文件后缀名就是封装格式的名字。不过我们也可以手动修改视频文件的后缀。

编码格式
       所谓视频编码格式就是指通过特定的压缩技术，将某个视频格式的文件转换成另一种视频格式文件的方式。不同编码方式的区别主要是压缩算法的不同。视频编码的目的主要是压缩数据体积。视频中常用的编码标准有H.26X系列、MPEG系列、Divx、Xvid、WMV-HD和VC-1等等。  
       在移动端使用最多的mp4视频文件，主要使用H.264 AVC编码格式。它主要有以下优点：
       1．低码率：和MPEG2、MPEG4等压缩技术相比，在同等图像质量下，采用H.264技术压缩后的数据量只有MPEG2的1/8，MPEG4的1/3。H.264的最大优势是具有很高的数据压缩率，可以有效减少用户带宽的占用。
       2．高质量的图像：H.264能提供连续、流畅的高质量图像。
       3．容错能力强：H.264提供了解决在不稳定网络环境下容易发生的丢包等错误的必要工具。
       4．网络适应性强：H.264提供了网络抽象层，使得H.264的文件能容易地在不同网络上传输。
       我们注意到，一个完整的视频文件是由音频和视频两部分组成。H.264、Xvid等就是视频编码格式，MP3、AAC等就是音频编码格式。而当前客户端中使用最多的mp4格式的视频文件，视频和音频编码格式通常使用的是H.264和AAC编码。
       下图列出来常用的视频封装格式与编码格式的对应关系

 
      事实上，很多封装容器对音频编码和视频编码的组合方式放的很开，如AVI还可以使用H264+AAC组合。尤
其是MKV封装容器，基本无论什么样的组合都可以！但一般MKV用的最多的就是H264+AAC组合，此组合文件
体积最小，清晰度最高。


H.264编码
基本原理
       H.264是新一代的编码标准，以高压缩高质量和支持多种网络的流媒体传输著称，在编码方面，它的理论依据是：一段时间内图像的统计结果表明，在相邻几幅图像画面中，一般有差别的像素只有10%以内的点，亮度差值变化不超过2%，而色度差值的变化只有1%以内。所以对于一段变化不大的图像画面，我们可以先编码出一个完整的图像帧A，随后的B帧不编码全部图像，只写入与A帧的差别，这样B帧的大小就只有完整帧的1/10或更小！B帧之后的C帧如果变化不大，我们可以继续以参考B的方式编码C帧，这样循环下去。这段图像我们称为一个序列，当某个图像与之前的图像变化很大，无法参考前面的帧来生成，那我们就结束上一个序列，开始下一段序列。    
　　在H264协议里定义了三种帧，完整编码的帧叫I帧，参考之前的I帧生成的只包含差异部分编码的帧叫P帧，还有一种参考前后的帧编码的帧叫B帧。    
　　H264采用的核心算法是帧内压缩和帧间压缩，帧内压缩是生成I帧的算法，帧间压缩是生成B帧和P帧的算法。

序列   
　　在H264中，图像以序列为单位进行组织，一个序列是一段图像编码后的数据流，以I帧开始，到下一个I帧结束。    
　　一个序列的第一个图像叫做 IDR 图像（立即刷新图像），IDR 图像都是 I 帧图像。H.264 引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码。        
　　一个序列就是一段内容差异不太大的图像编码后生成的一串数据流。当运动变化比较少时，一个序列可以很长，因为运动变化少就代表图像画面的内容变动很小，所以就可以编一个I帧，然后一直P帧、B帧了。当运动变化多时，可能一个序列就比较短了，比如就包含一个I帧和3、4个P帧。

三种帧的不同
I帧 
帧内编码帧，I帧表示关键帧，可以理解为这一帧画面的完整保留，解码时只需要本帧数据就可以完成。
1)它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输;
2)解码时仅用I帧的数据就可重构完整图像;
3)I帧描述了图像背景和运动主体的详情;
4)I帧不需要参考其他画面而生成;
5)I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量);
6)I帧是帧组GOP的基础帧(第一帧)，在一组中只有一个I帧;
7)I帧不需要考虑运动矢量;
8)I帧所占数据的信息量比较大。
P帧 
前向预测编码帧。P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）。
P帧的预测与重构：P帧是以I帧为参考帧，在I帧中找出P帧“某点”的预测值和运动矢量，取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值从而可得到完整的P帧。
1)P帧是I帧后面相隔1~2帧的编码帧;
2)P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量(预测误差);
3)解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像;
4)P帧属于前向预测的帧间编码。它只参考前面最靠近它的I帧或P帧;
5)P帧可以是其后面P帧的参考帧，也可以是其前后的B帧的参考帧;
6)由于P帧是参考帧，它可能造成解码错误的扩散;
7)由于是差值传送，P帧的压缩比较高。
B帧
双向预测内插编码帧。B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累。
B帧的预测与重构：B帧以前面的I或P帧和后面的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。
1）B帧是由前面的I或P帧和后面的P帧来进行预测的;
2）B帧传送的是它与前面的I或P帧和后面的P帧之间的预测误差及运动矢量;
3）B帧是双向预测编码帧;
4）B帧压缩比最高,因为它只反映丙参考帧间运动主体的变化情况,预测比较准确;
5）B帧不是参考帧,不会造成解码错误的扩散。
       I、B、P各帧是根据压缩算法的需要，是人为定义的，它们都是实实在在的物理帧。一般来说，I帧的压缩率是7，P帧是20，B帧可以达到50。可见使用B帧能节省大量空间，节省出来的空间可以用来保存多一些I帧，这样在相同码率下，可以提供更好的画质。
       在视频编码序列中，GOP即Group of Picture（图像组），指两个I帧之间的距离，Reference（参考周期）指两个P帧之间的距离。一个I帧所占用的字节数大于一个P帧，一个P帧所占用的字节数大于一个B帧。所以，在码率不变的前提下，GOP值越大，P、B帧的数量会越多，画面细节更多，也就更容易获取较好的图像质量；Reference越大，B帧的数量越多，同理也更容易获得较好的图像质量。
       由于P、B帧的复杂度大于I帧，所以过多的P、B帧会影响编码效率，使编码效率降低。另外，过长的GOP还会影响Seek操作（找I帧）的响应速度，由于P、B帧是由前面的I或P帧预测得到的，所以Seek操作需要直接定位，解码某一个P或B帧时，需要先解码得到本GOP内的I帧及之前的N个预测帧才可以，GOP值越长，需要解码的预测帧就越多，seek响应的时间也越长。  假设I帧损坏，那么整个GOP结构就坏掉了，设置过长的间隔也会导致编码后的视频不稳定。
       一个 I 帧可以不依赖其他帧就解码出一幅完整的图像，而 P 帧、B 帧不行。P 帧需要依赖视频流中排在它前面的帧才能解码出图像。B 帧则需要依赖视频流中排在它前面或后面的帧才能解码出图像。
       这就带来一个问题：在视频流中，先到来的 B 帧无法立即解码，需要等待它依赖的后面的P 帧先解码完成，这样一来播放时间与解码时间不一致了，顺序打乱了，那这些帧该如何播放呢？这时就需要我们来了解另外两个概念：DTS 和 PTS。

       DTS和PTS 

       DTS（Decoding Time Stamp）：即解码时间戳，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。
       PTS（Presentation Time Stamp）：即显示时间戳，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。
       需要注意的是：虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。
       当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，解码顺序和播放顺序就不一致了。
       比如一个视频中，帧的显示顺序是：I B B P，现在我们需要在解码 B 帧时知道 P 帧中信息，因此这几帧在视频流中的顺序可能是：I P B B，这时候就体现出每帧都有 DTS 和 PTS 的作用了。它们的顺序大概如下：



       音视频同步

       上面说了DTS、PTS 相关的概念。在一个媒体流中，除了视频以外，通常还包括音频。音频的播放，也有 DTS、PTS 的概念，但是音频没有类似视频中 B 帧，不需要双向预测，所以音频帧的 DTS、PTS 顺序是一致的。音频视频混合在一起播放，就呈现了完整的视频。在音视频一起播放的时候，我们通常需要面临怎么去同步它们的问题，以免出现音画不同步的情况。要实现音视频同步，通常需要选择一个参考时钟，参考时钟上的时间是线性递增的，编码音视频流时依据参考时钟上的时间给每帧数据打上时间戳。在播放时，读取数据帧上的时间戳，同时参考当前参考时钟上的时间来安排播放。这里的说的时间戳就是我们前面说的 PTS。

压缩算法
H.264的压缩方法
       1.分组:把几帧图像分为一组(GOP，也就是一个序列)，为防止运动变化，帧数不宜取多。
       2.定义帧:将每组内各帧图像定义为三种类型,即I帧、B帧和P帧;
       3.预测帧:以I帧做为基础帧,以I帧预测P帧,再由I帧和P帧预测B帧;
       4.数据传输:最后将I帧数据与预测的差值信息进行存储和传输。
帧内压缩和帧间压缩
       帧内（Intraframe）压缩也称为空间压缩。当压缩一帧图像时，仅考虑本帧的数据而不考虑相邻帧之间的冗余信息，这实际上与静态图像压缩类似。帧内一般采用有损压缩算法，由于帧内压缩是编码一个完整的图像，所以可以独立的解码、显示。帧内压缩一般达不到很高的压缩。　　    
       帧间（Interframe）压缩的原理是：相邻几帧的数据有很大的相关性，或者说前后两帧信息变化很小的特点。也即连续的视频其相邻帧之间具有冗余信息,根据这一特性，压缩相邻帧之间的冗余量就可以进一步提高压缩量，减小压缩比。帧间压缩也称为时间压缩（Temporal compression），它通过比较时间轴上不同帧之间的数据进行压缩。帧间压缩一般是无损的。帧差值（Frame differencing）算法是一种典型的时间压缩法，它通过比较本帧与相邻帧之间的差异，仅记录本帧与其相邻帧的差值，这样可以大大减少数据量。  

      本文关于视频开发基础的内容就介绍这么多，下一篇文章将会介绍视频开发在Android上的应用以及开发中遇到的一些问题，希望能够对大家有所帮助。